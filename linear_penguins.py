# Шаг 0. Импорты и настройки
import numpy as np              # Импорт библиотеки для численных вычислений
import pandas as pd             # Импорт библиотеки для работы с табличными данными
import matplotlib.pyplot as plt # Импорт библиотеки для построения графиков
import stats as stats
from scipy import stats
from scipy.stats import skew, kurtosis
from sklearn.linear_model import LinearRegression   # Импорт класса линейной регрессии из scikit-learn
from sklearn.metrics import accuracy_score, confusion_matrix, ConfusionMatrixDisplay    # Импорт метрик для классификации
plt.rcParams['figure.figsize'] = (6, 4)         # Настройка размера графиков по умолчанию (ширина 6 дюймов, высота 4 дюйма)
np.set_printoptions(suppress=True, precision=6) # Настройка отображения чисел в numpy, вывод чисел с 6 знаками после запятой
pd.set_option('display.precision', 6)           #То же, что и предыдущее

# Шаг 1. Укажите путь к вашему CSV и нужные колонки
# Заполните переменные ниже: CSV_PATH, FEATURE_COLS (список признаков), TARGET_COL (целевой).
CSV_PATH = 'dataset.xlsx'  # путь к файлу с данными пингвинов
FEATURE_COLS = ['x1', 'x2', 'x3', 'x4']  # признаки: x1=длина клюва, x2=глубина клюва, x3=длина плавника, x4=масса тела
TARGET_COL = 'y'  # целевая переменная: y=вид пингвина

# Шаг 2. Загрузка данных и базовая проверка
df = pd.read_excel(CSV_PATH)
assert all(col in df.columns for col in FEATURE_COLS), 'Не найдены все признаки в таблице'  # assert проверяет истинность условия
assert TARGET_COL in df.columns, 'Не найден столбец TARGET_COL'
# Попробуем привести y к числу (если классы записаны строками, преобразуйте их заранее)
df[TARGET_COL] = pd.to_numeric(df[TARGET_COL], errors='coerce')
assert df[TARGET_COL].notna().all(), 'В y должны быть числа (при необходимости предварительно закодируйте классы)'
df.head()

# Шаг 3. Собираем матрицы X, y и обучаем LinearRegression (= LINEST)
# Получите ŷ и остатки resid = y - ŷ.
# Формируем матрицу признаков X из выбранных столбцов и преобразуем в numpy массив
X = df[FEATURE_COLS].to_numpy()  # Берем колонки x1, x2, x3, x4 и превращаем в числовую матрицу
# Формируем вектор целевой переменной y и преобразуем в float
y = df[TARGET_COL].to_numpy().astype(float)  # Берем колонку y (вид пингвина) и делаем числами
# Создаем и обучаем модель линейной регрессии с включением свободного члена
model = LinearRegression(fit_intercept=True).fit(X, y)  # Обучаем модель на данных X и y
# Получаем предсказанные значения ŷ с помощью обученной модели
y_hat = model.predict(X)  # Модель предсказывает вид пингвина по признакам
# Вычисляем остатки - разницу между реальными и предсказанными значениями
resid = y - y_hat  # Остатки = настоящий вид минус предсказанный вид

# Самопроверка A
n, k = X.shape  # Получаем размеры матрицы X: n - количество строк (наблюдений), k - количество столбцов (признаков)
assert y.shape == (n,), 'y должен быть вектором длины n'  # Проверяем, что размер вектора y совпадает с количеством наблюдений
assert k >= 1, 'Нужно минимум 1 признак'  # Проверяем, что в модели есть хотя бы один признак
print('OK: n=%d, k=%d' % (n, k))  # Выводим информацию о размерах данных: количество наблюдений и признаков

# Шаг 4. Базовые суммы квадратов и метрики LINEST
# Посчитайте:

# SStot = Σ (y - ȳ)² - общая сумма квадратов (общая изменчивость данных) (y факт - ср знач y)
SStot = ((y - y.mean())**2).sum()  # Сумма квадратов отклонений реальных значений от среднего

# SSresid = Σ (y - ŷ)² - сумма квадратов остатков (необъясненная моделью изменчивость)
SSresid = ((y - y_hat)**2).sum()  # Сумма квадратов ошибок предсказания

# SSreg = SStot - SSresid - разница между общей и остаточной изменчивостью (объясненная моделью изменчивость)
SSreg = SStot - SSresid  # Часть изменчивости, которую смогла объяснить модель

# R² = 1 - SSresid/SStot - коэффициент детерминации (доля объясненной дисперсии)
R2 = 1 - SSresid / SStot  # Показывает, насколько хорошо модель описывает данные (от 0 до 1)

# df = n - k - 1 - степени свободы остатков
df_resid = n - k - 1  # Количество свободно изменяемых наблюдений после учета параметров модели

# SE_y = √(SSresid/df) - стандартная ошибка оценки
SE_y = np.sqrt(SSresid / df_resid)  # Средняя ошибка предсказания модели

# F = (SSreg/k)/(SSresid/df) - F-статистика для проверки значимости модели
F = (SSreg / k) / (SSresid / df_resid)  # Проверяет, значима ли модель в целом
print("Метрики модели рассчитаны:")
print(f"SStot = {SStot:.2f}")      # Общая изменчивость
print(f"SSreg = {SSreg:.2f}")      # Объясненная изменчивость
print(f"SSresid = {SSresid:.2f}")  # Необъясненная изменчивость
print(f"R² = {R2:.4f}")           # Качество модели (0-1)
print(f"Степени свободы = {df_resid}")  # Надежность оценок
print(f"SE_y = {SE_y:.2f}")       # Средняя ошибка предсказания
print(f"F-статистика = {F:.2f}")  # Значимость модели
#ПРОВЕРОЧКА
assert df_resid == n - k - 1, 'df должно быть n - k - 1'
assert SStot >= 0 and SSresid >= 0 and SSreg >= 0, 'Суммы квадратов не могут быть отрицательными'
assert 0 <= R2 <= 1 or np.isclose(R2, 1.0) or np.isclose(R2, 0.0), 'R^2 должен быть в [0,1] на тренировочных данных'
print('OK')

# Шаг 5. SE коэффициентов (2-я строка LINEST)
# Соберите дизайн-матрицу X1 = [X, 1] и используйте формулу:
# Var(β̂) = σ²(X₁ᵀX₁)⁻¹, где σ² = SS_resid/df

# Создаем расширенную матрицу признаков X1, добавляя столбец единиц для свободного члена, т.е. b
X1 = np.column_stack([X, np.ones(n)])  # Дизайн-матрица: [признаки, единицы]

# Вычисляем матрицу X1 транспонированную умноженную на X1
XtX = X1.T @ X1  # Матрица скалярных произведений признаков

# Пытаемся найти обратную матрицу (X1ᵀX1)⁻¹
try:
    XtX_inv = np.linalg.inv(XtX)  # Стандартное обращение матрицы
except np.linalg.LinAlgError:
    XtX_inv = np.linalg.pinv(XtX)  # Псевдообратная матрица, если матрица вырождена

# Вычисляем оценку дисперсии ошибок σ²
sigma2 = SSresid / df_resid  # Несмещенная оценка дисперсии: остаточная сумма квадратов / степени свободы

# Вычисляем стандартные ошибки коэффициентов как корень из диагональных элементов ковариационной матрицы
se_beta = np.sqrt(np.diag(sigma2 * XtX_inv))  # SE(b) = √[σ² · diag((X₁ᵀX₁)⁻¹)]

print("Стандартные ошибки коэффициентов:")
print(f"SE для x1 (длина клюва): {se_beta[0]:.6f}")
print(f"SE для x2 (глубина клюва): {se_beta[1]:.6f}")
print(f"SE для x3 (длина плавника): {se_beta[2]:.6f}")
print(f"SE для x4 (масса тела): {se_beta[3]:.6f}")
print(f"SE для intercept (b): {se_beta[4]:.6f}")

# Шаг 6. Собираем пятирядный блок как в Excel
# 1-я строка: коэффициенты {m_k, …, m_1, b0} (обратите внимание на обратный порядок признаков, как в LINEST)
# 2-я: SE в том же порядке
# 3-я: {R², SE_y, NA, NA, NA}
# 4-я: {F, df, NA, NA, NA}
# 5-я: {SSreg, SSresid, NA, NA, NA}

# Получаем коэффициенты модели и свободный член
coefs = getattr(model, 'coef_', None)  # Коэффициенты при признаках [m1, m2, m3, m4], getattr вытаскивает из машинной модели наши коэффициенты
intercept = getattr(model, 'intercept_', None)  # Свободный член β₀ (b0)

# Создаем DataFrame с 5 строками и k+1 столбцами (4 признака + intercept)
linest_like = pd.DataFrame(index=[1,2,3,4,5], columns=[f'c{i+1}' for i in range(k+1)])

# Заполняем строки 1 и 2 с учетом обратного порядка коэффициентов (как в Excel LINEST)
rev_coefs = list(coefs[::-1]) + [intercept]  # Коэффициенты в обратном порядке: [m4, m3, m2, m1, b]
rev_se = list(se_beta[:k][::-1]) + [se_beta[-1]]  # SE коэффициентов в том же порядке: [SE(m4), SE(m3), SE(m2), SE(m1), SE(b)]

# Записываем коэффициенты и их стандартные ошибки
linest_like.loc[1, :] = rev_coefs  # Строка 1: коэффициенты регрессии
linest_like.loc[2, :] = rev_se     # Строка 2: стандартные ошибки коэффициентов

# Строки 3–5: только первые 2 столбца, остальные — NaN (как #Н/Д в Excel)
linest_like.loc[3, :] = [R2, SE_y] + [np.nan] * (k-1)  # Строка 3: R^2 и стандартная ошибка модели
linest_like.loc[4, :] = [F, df_resid] + [np.nan] * (k-1)  # Строка 4: F-статистика и степени свободы
linest_like.loc[5, :] = [SSreg, SSresid] + [np.nan] * (k-1)  # Строка 5: суммы квадратов
print("Пятирядный блок в формате Excel LINEST:")
print(linest_like)

# Расчет t-статистики и p-value
# t-статистика = коэффициент / его стандартная ошибка
t_stats = model.coef_/ se_beta[:-1]  # se_beta[:-1] - SE для коэффициентов (без intercept)
# p-value для каждого коэффициента (двусторонний тест)
p_values = 2 * (1 - stats.t.cdf(np.abs(t_stats), df_resid))
# Выводим результаты для каждого признака
print("\nКоэффициенты и их значимость:")
for i, col in enumerate(FEATURE_COLS):
    significance = "- ЗНАЧИМ" if p_values[i] < 0.05 else "- НЕЗНАЧИМ"
    print(f"{col}: coef = {model.coef_[i]:.6f}, t = {t_stats[i]:.4f}, p-value = {p_values[i]:.6f} {significance}")
# Для свободного члена
t_intercept = intercept / se_beta[-1]
p_intercept = 2 * (1 - stats.t.cdf(np.abs(t_intercept), df_resid))
significance_int = "- ЗНАЧИМ" if p_intercept < 0.05 else "- НЕЗНАЧИМ"
print(f"intercept: coef = {intercept:.6f}, t = {t_intercept:.4f}, p-value = {p_intercept:.6f} {significance_int}")
# Сводка по значимости
significant_features = [FEATURE_COLS[i] for i in range(len(FEATURE_COLS)) if p_values[i] < 0.05]
print(f"\nСтатистически значимые факторы (p < 0.05): {significant_features}")

# Шаг 7. (Опционально) Если y — целые классы, округлите ŷ и посчитайте accuracy
# Установите TREAT_AS_CLASSES = True, если хотите режим «классы через округление». Если у вас обычная регрессия — оставьте False.

TREAT_AS_CLASSES = True  # Меняем на True, так как у меня y — целые классы (виды пингвинов 1,2,3)

if TREAT_AS_CLASSES:
    # Определяем минимальное и максимальное значение классов
    y_min, y_max = int(np.nanmin(y)), int(np.nanmax(y))  # y_min=1, y_max=3 (виды пингвинов)
    y_pred_class = np.clip(np.rint(y_hat), y_min, y_max).astype(
        int)  # Округляем ŷ к ближайшему целому и обрезаем до диапазона [1,3], ошибку для внимательных учли, np.clip() — это функция в библиотеке NumPy для Python, которая обрезает значения в массиве или отдельном числе, чтобы они находились в заданном диапазоне от минимального до максимального значения.
    # Вычисляем точность (accuracy) - доля правильных предсказаний
    acc = accuracy_score(y.astype(int), y_pred_class)  # Сравниваем реальные классы с предсказанными

    print('Accuracy:', round(acc, 4))  # Выводим точность с округлением до 4 знаков

#Шаг 8. (Опционально) Матрица ошибок и два графика
#if TREAT_AS_CLASSES:
 #   cm = confusion_matrix(y.astype(int), y_pred_class, labels=sorted(np.unique(y_pred_class)))
  #  disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=sorted(np.unique(y_pred_class)))
   # disp.plot(); plt.title('Матрица ошибок'); plt.show()

#plt.figure(); plt.scatter(y, y_hat); mn, mx = np.nanmin([y.min(), y_hat.min()]), np.nanmax([y.max(), y_hat.max()]); plt.plot([mn,mx],[mn,mx]); plt.xlabel('y'); plt.ylabel('ŷ'); plt.title('y vs ŷ'); plt.show()
#plt.figure(); plt.hist(y - y_hat, bins=20); plt.xlabel('Остаток'); plt.ylabel('Частота'); plt.title('Распределение остатков'); plt.show()

#Шаг 9. Посчитать MSE
from sklearn.metrics import mean_squared_error  #Библиотека sklearn добавили функцию для расчета MSE
MSE = mean_squared_error(y, y_hat)
print('MSE = ', MSE)                            #Показывает реднюю величину ошибки предсказания, т.е. в среднем наши предсказания отличаются от реальных значений на √0,17 на ~ 0,42 единицы

#Шаг 10. Расчет требуемых показателей для исходной модели - Системный эффект, мера мультиколлинеарности
print("\nПОКАЗАТЕЛЬ СИСТЕМНОГО ЭФФЕКТА ФАКТОРОВ")
# Бета-коэффициенты (стандартизированные коэффициенты регрессии)
sigma_x = X.std(axis=0, ddof=1)  # стандартное отклонение факторов, большее σₓ = больше вариативность фактора, если x1 - длина клюва, σₓ₁ = 2.5 мм означает, что длины клювов в среднем отличаются на 2.5 мм
sigma_y = y.std(ddof=1)          # стандартное отклонение целевой переменной, насколько виды различаются
beta_coefs = model.coef_ * (sigma_x / sigma_y)
print(f"Бета-коэффициенты:")
for i, col in enumerate(FEATURE_COLS):
    print(f"  {col}: β = {beta_coefs[i]:.4f}")  #Вывод: Масса тела имеет наибольшее влияние на вид пингвина, поскольку бета к-ф больше остальных
# Сумма квадратов бета-коэффициентов
sum_beta_squared = (beta_coefs**2).sum()
print(f"Сумма квадратов β-коэффициентов: {sum_beta_squared:.4f}")
# Показатель системного эффекта факторов
# η_s = R² - Σβ_j² (показывает совместное влияние факторов на y без учета внутренних связей)
systematic_effect = R2 - sum_beta_squared
print(f"R² модели: {R2:.4f}")
print(f"Показатель системного эффекта факторов η_s = {systematic_effect:.4f}")
# Интерпретация
if systematic_effect > 0:
    print("Положительный системный эффект - факторы совместно усиливают влияние")
elif systematic_effect < 0:
    print("Отрицательный системный эффект - присутствует мультиколлинеарность")
else:
    print("Нулевой системный эффект - факторы независимы")

#Мера мультиколлинеарности
print("\nМЕРА МУЛЬТИКОЛЛИНЕАРНОСТИ")
#Матрица корреляций
corr_matrix = df[FEATURE_COLS + [TARGET_COL]].corr()
print("Матрица корреляций:")                            #квадратная таблица, показывающая степень и направление взаимосвязи между всеми парами переменных в наборе данных.
print(corr_matrix.round(4))                             #где +1 означает сильную положительную связь, -1 — сильную отрицательную, а 0 — отсутствие связи, если 0,8, надо задуматься об исключении фактора

# Мера мультиколлинеарности по презентации M = D - Σd_yj
# где D = R², d_yj = r_yj² (квадраты коэффициентов парной корреляции с y), R² - доля объясненной дисперсии моделью, r² - доля объясненной дисперсии каждым фактором в отдельности
D = R2
d_yj = np.array([corr_matrix.loc[TARGET_COL, col]**2 for col in FEATURE_COLS])  #к-ф парной детерминации
multicollinearity_measure = D - d_yj.sum()
print(f"\nR² модели (D): {D:.4f}")
print(f"Сумма квадратов парных корреляций с y (Σd_yj): {d_yj.sum():.4f}")
print(f"Мера мультиколлинеарности M = D - Σd_yj = {multicollinearity_measure:.4f}")

#Введем привычную меру мультиколлинеарности, VIF - к-ф инфляции дисперсии - глвный метод меры мультиколлинеарности
VIF = 1/1-D
print(f"Мера мультиколлинеарности VIF: {VIF:.4f}")  #Чем выше значение VIF, тем сильнее мультиколлинеарность, в нашем случае умеренная мультиколлинеарность (1 - идеальная)

# Шаг 11. Проделываем все то же самое с новой моделью. Исключила фактор x4, поскольку по SE критерий статистически незначимый и в матрице корреляций он мультиколлинеарен с х3
print("\n--------------НОВЫЙ ДАТАСЕТ--------------")
CSV_PATH_NEW = 'dataset_new.xlsx'  # путь к файлу с данными пингвинов
FEATURE_COLS_NEW = ['x1', 'x2', 'x3']  # признаки: x1=длина клюва, x2=глубина клюва, x3=длина плавника
TARGET_COL_NEW = 'y'  # целевая переменная: y=вид пингвина

df_new = pd.read_excel(CSV_PATH_NEW)
assert all(col in df_new.columns for col in FEATURE_COLS_NEW), 'Не найдены все признаки в таблице'  # assert проверяет истинность условия
assert TARGET_COL_NEW in df_new.columns, 'Не найден столбец TARGET_COL'
df_new[TARGET_COL_NEW] = pd.to_numeric(df_new[TARGET_COL_NEW], errors='coerce')
assert df_new[TARGET_COL_NEW].notna().all(), 'В y должны быть числа (при необходимости предварительно закодируйте классы)'
df_new.head()

X_new = df_new[FEATURE_COLS_NEW].to_numpy()  # Берем колонки x1, x2, x3 и превращаем в числовую матрицу
y_new = df_new[TARGET_COL_NEW].to_numpy().astype(float)  # Берем колонку y (вид пингвина) и делаем числами
model_new = LinearRegression(fit_intercept=True).fit(X_new, y_new)  # Обучаем модель на данных X и y
y_hat_new = model_new.predict(X_new)  # Модель предсказывает вид пингвина по признакам
resid_new = y_new - y_hat_new  # Остатки = настоящий вид минус предсказанный вид
# Самопроверка A
n, k_new = X_new.shape  # Получаем размеры матрицы X: n - количество строк (наблюдений), k - количество столбцов (признаков)
assert y_new.shape == (n,), 'y должен быть вектором длины n'  # Проверяем, что размер вектора y совпадает с количеством наблюдений
assert k_new >= 2, 'Нужно минимум 2 признака'  # Проверяем, что в модели есть хотя бы два признака
print('OK: n=%d, k=%d' % (n, k))  # Выводим информацию о размерах данных: количество наблюдений и признаков
SStot_new = ((y_new - y_new.mean())**2).sum()  # Сумма квадратов отклонений реальных значений от среднего
SSresid_new = ((y_new - y_hat_new)**2).sum()  # Сумма квадратов ошибок предсказания
SSreg_new = SStot_new - SSresid_new  # Часть изменчивости, которую смогла объяснить модель
R2_new = 1 - SSresid_new / SStot_new  # Показывает, насколько хорошо модель описывает данные (от 0 до 1)
df_resid_new = n - k_new - 1  # Количество свободно изменяемых наблюдений после учета параметров модели
SE_y_new = np.sqrt(SSresid_new / df_resid_new)  # Средняя ошибка предсказания модели
F_new = (SSreg_new / k_new) / (SSresid_new / df_resid_new)  # Проверяет, значима ли модель в целом
print("Метрики модели рассчитаны:")
print(f"SStot = {SStot_new:.2f}")      # Общая изменчивость
print(f"SSreg = {SSreg_new:.2f}")      # Объясненная изменчивость
print(f"SSresid = {SSresid_new:.2f}")  # Необъясненная изменчивость
print(f"R² = {R2_new:.4f}")           # Качество модели (0-1)
print(f"Степени свободы = {df_resid_new}")  # Надежность оценок
print(f"SE_y = {SE_y_new:.2f}")       # Средняя ошибка предсказания
print(f"F-статистика = {F_new:.2f}")  # Значимость модели

print("\nРАСЧЕТЫ ДЛЯ НОВОГО ДАТАСЕТА")
from sklearn.metrics import mean_squared_error  #Библиотека sklearn добавили функцию для расчета MSE
MSE_new = mean_squared_error(y_new, y_hat_new)
print('MSE new = ', MSE_new)                            #Показывает реднюю величину ошибки предсказания, т.е. в среднем наши предсказания отличаются от реальных значений на √0,17 на ~ 0,42 единицы
sigma_x_new = X_new.std(axis=0, ddof=1)  # стандартное отклонение факторов, большее σₓ = больше вариативность фактора, если x1 - длина клюва, σₓ₁ = 2.5 мм означает, что длины клювов в среднем отличаются на 2.5 мм
sigma_y_new = y_new.std(ddof=1)          # стандартное отклонение целевой переменной, насколько виды различаются
beta_coefs_new = model_new.coef_ * (sigma_x_new / sigma_y_new)
print(f"Бета-коэффициенты new:")
for i, col in enumerate(FEATURE_COLS_NEW):
    print(f"  {col}: β = {beta_coefs_new[i]:.4f}")
# Сумма квадратов бета-коэффициентов
sum_beta_squared_new = (beta_coefs_new**2).sum()
print(f"Сумма квадратов β-коэффициентов new: {sum_beta_squared_new:.4f}")
# Показатель системного эффекта факторов
# η_s = R² - Σβ_j² (показывает совместное влияние факторов на y без учета внутренних связей)
systematic_effect_new = R2_new - sum_beta_squared_new
print(f"R² модели: {R2_new:.4f}")
print(f"Показатель системного эффекта факторов η_s = {systematic_effect_new:.4f}")
# Интерпретация
if systematic_effect_new > 0:
    print("Положительный системный эффект - факторы совместно усиливают влияние")
elif systematic_effect_new < 0:
    print("Отрицательный системный эффект - присутствует мультиколлинеарность")
else:
    print("Нулевой системный эффект - факторы независимы")

corr_matrix_new = df_new[FEATURE_COLS_NEW + [TARGET_COL_NEW]].corr()
print("Матрица корреляций:")                            #квадратная таблица, показывающая степень и направление взаимосвязи между всеми парами переменных в наборе данных.
print(corr_matrix_new.round(4))                             #где +1 означает сильную положительную связь, -1 — сильную отрицательную, а 0 — отсутствие связи, если 0,8, надо задуматься об исключении фактора
# Мера мультиколлинеарности по презентации M = D - Σd_yj
# где D = R², d_yj = r_yj² (квадраты коэффициентов парной корреляции с y), R² - доля объясненной дисперсии моделью, r² - доля объясненной дисперсии каждым фактором в отдельности
D_new = R2_new
d_yj_new = np.array([corr_matrix_new.loc[TARGET_COL_NEW, col]**2 for col in FEATURE_COLS_NEW])  #к-ф парной детерминации
multicollinearity_measure_new = D_new - d_yj_new.sum()
print(f"\nR² модели (D): {D_new:.4f}")
print(f"Сумма квадратов парных корреляций с y (Σd_yj): {d_yj_new.sum():.4f}")
print(f"Мера мультиколлинеарности M = D - Σd_yj = {multicollinearity_measure_new:.4f}, значение уменьшилось, по сравнению со старой моделью")

#Введем привычную меру мультиколлинеарности, VIF - к-ф инфляции дисперсии - глвный метод меры мультиколлинеарности
VIF_new = 1/1-D_new
print(f"Мера мультиколлинеарности VIF new: {VIF_new:.4f}")
print("В целом исключение фактора х4 было целесообразным на первый взгляд, модель не потеряла в прогнозной силе и вырос F критерий, избавились от проблемы мультиколлинеарности факторов х3 и х4 (|r| > 0,8).")

#Шаг 12. Критерий Фишера на целесообразность исключения фактора
print("\nПРОВЕРКА ЦЕЛЕСООБРАЗНОСТИ ИСКЛЮЧЕНИЯ ФАКТОРОВ")
print("Критерий Фишера по точной формуле")
# Исходная модель (полная): с факторами x1, x2, x3, x4
# Новая модель (упрощенная): с факторами x1, x2, x3
# Данные из предыдущих расчетов:
D_m = R2          # Коэффициент детерминации полной модели: 0.8289
D_m1 = R2_new     # Коэффициент детерминации упрощенной модели: 0.8285
n = n             # Количество наблюдений: 300
m = k             # Количество факторов в полной модели: 4
m1 = k_new        # Количество факторов в упрощенной модели: 3
# Расчет по формуле в презентации:
# F₁ = [(D_m - D_m1) × (n - m - 1)] / [(m - m1) × (1 - D_m)]
m2 = m - m1  # Количество исключенных факторов: 1
f1 = m2      # Первая степень свободы: 1
f2 = n - m - 1  # Вторая степень свобода: 295
numerator = (D_m - D_m1) * (n - m - 1)
denominator = (m - m1) * (1 - D_m)
F_observed = numerator / denominator
print('Fнабл. = ', F_observed)

# Критическое значение распределения Фишера
alpha = 0.05                                # Уровень значимости
from scipy.stats import f, stats

F_critical = f.ppf(1 - alpha, f1, f2)
print(f"Критическое значение:")
print(f"Fкрит({f1}, {f2}) при α={alpha}: {F_critical:.4f}")
# P-value для наблюдаемой F-статистики, чем меньше p-значение, тем более значимым будет результат, потому что он с меньшей вероятностью будет вызван шумом
p_value = 1 - f.cdf(F_observed, f1, f2)                     # вероятность получить такие же или более крайние результаты
print(f"P-value: {p_value:.6f}")                            # Нулевая гипотеза (H0): "Исключенный фактор x4 НЕ вносит существенный вклад", в выводе нет статистических доказательств того, что фактор x4 вносит существенный вклад в модель, так что Н0 не отвергаем. Вероятность получить такие же результаты, если х4 не нужен - 82%
print("\nСТАТИСТИЧЕСКИЙ ВЫВОД:")
print(f"Fнабл = {F_observed:.4f}, Fкрит = {F_critical:.4f}")
if F_observed > F_critical:
    print("Fнабл > Fкрит")
    print("m2 объясняющих переменных СОВМЕСТНО оказывают СУЩЕСТВЕННОЕ влияние")
    print("Все m2 переменные из модели НЕЛЬЗЯ ИСКЛЮЧАТЬ")
else:
    print("Fнабл ≤ Fкрит")
    print("m2 переменных СОВМЕСТНО НЕ оказывают существенного влияния")
    print("Эти переменные окончательно ИСКЛЮЧАЮТСЯ из модели")
print(f"УРОВЕНЬ ЗНАЧИМОСТИ:")
if p_value < alpha:
    print(f"p-value ({p_value:.4f}) < α ({alpha}) - различия статистически значимы")
else:
    print(f"p-value ({p_value:.4f}) > α ({alpha}) - различия статистически незначимы")
print("ОКОНЧАТЕЛЬНЫЙ ВЫВОД: исключение фактора х4 является целесообразным. Я ТАК УСТАЛА")

# Шаг 13. Полная проверка условий Гаусса-Маркова
print("\n--------------ПРОВЕРКА УСЛОВИЙ ГАУССА-МАРКОВА--------------")
# Используем остатки из новой модели (без x4)
print("Гипотеза 1: Сумма ошибки равна нулю")
residuals = resid_new
n = len(residuals)
e = residuals
mean_e = e.mean()
std_e = e.std(ddof=1)
s = std_e  # стандартное отклонение остатков
# t-критерий по формуле из презентации
t_p = (np.sum(e) / n) * np.sqrt(n / s) if s != 0 else 0
f = n - 1  # степени свободы
from scipy.stats import t
t_critical = t.ppf(0.975, f)  # для α=0.05, двусторонний тест
print(f"Среднее остатков: {mean_e:.20f}")
print(f"Стандартное отклонение остатков: {s:.8f}")
print(f"t-статистика: {t_p:.20f}")
print(f"Критическое значение t({f}, α=0.05): {t_critical:.8f}")
if abs(t_p) <= t_critical:
    print("Гипотеза принимается: сумма ошибок равна нулю")
else:
    print("Гипотеза отвергается: сумма ошибок не равна нулю")

print("\nГипотеза 2: Автокорреляция в ряде остатков отсутствует")
# Вычисляем остатки модели (разницы между реальными и предсказанными значениями)
residuals = y - y_hat
# Критерий Дарбина-Уотсона для проверки независимости остатков
# Формула: DW = Σ(Δεᵢ)² / Σεᵢ², где Δεᵢ = εᵢ - εᵢ₋₁
dw = np.sum(np.diff(residuals)**2) / np.sum(residuals**2)  # Сумма квадратов разностей остатков деленная на сумму квадратов остатков
print(f"Критерий Дарбина-Уотсона = {dw:.4f}")
# Интерпретация критерия Дарбина-Уотсона
if dw < 1.5:
    print("Положительная автокорреляция остатков")  # Последовательные остатки коррелируют
elif dw > 2.5:
    print("Отрицательная автокорреляция остатков")  # Остатки чередуются знаками
else:
    print("Нет автокорреляции остатков")  # Остатки независимы
print("Гипотеза принимается: автокорреляция в ряде остатков отсутствует")

from scipy.stats import skew, kurtosis
print("\nГипотеза 3: Нормальное распределение остатков (гомоскедастичность)")
# Расчет асимметрии распределения остатков
# Асимметрия = 0 для симметричного распределения, >0 - правый хвост, <0 - левый хвост
skewness = skew(residuals)  # Функция вычисляет коэффициент асимметрии
print(f"Асимметрия остатков = {skewness:.4f}")
# Расчет эксцесса распределения остатков
# Эксцесс = 0 для нормального распределения, >0 - острый пик, <0 - плоский пик
kurtosis = kurtosis(residuals)  # Функция вычисляет коэффициент эксцесса
print(f"Эксцесс остатков = {kurtosis:.4f}")
# Интерпретация асимметрии
if abs(skewness) < 0.5:
    print("Распределение остатков симметрично")  # Распределение близко к нормальному
elif skewness > 0.5:
    print("Положительная асимметрия (хвост вправо)")  # Большие положительные выбросы
else:
    print("Отрицательная асимметрия (хвост влево)")  # Большие отрицательные выбросы
# Интерпретация эксцесса
if abs(kurtosis) < 1:
    print("Нормальный эксцесс (распределение близко к нормальному)")  # Нормальное распределение
elif kurtosis > 1:
    print("Положительный эксцесс (островершинное распределение)")
else:
    print("Отрицательный эксцесс (плосковершинное распределение)")  # Равномерное распределение
print("Гипотеза принимается: распределение близко к нормальному)")

