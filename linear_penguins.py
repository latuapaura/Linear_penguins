# Шаг 0. Импорты и настройки
import numpy as np              # Импорт библиотеки для численных вычислений
import pandas as pd             # Импорт библиотеки для работы с табличными данными
import matplotlib.pyplot as plt # Импорт библиотеки для построения графиков
from sklearn.linear_model import LinearRegression   # Импорт класса линейной регрессии из scikit-learn
from sklearn.metrics import accuracy_score, confusion_matrix, ConfusionMatrixDisplay    # Импорт метрик для классификации
plt.rcParams['figure.figsize'] = (6, 4)         # Настройка размера графиков по умолчанию (ширина 6 дюймов, высота 4 дюйма)
np.set_printoptions(suppress=True, precision=6) # Настройка отображения чисел в numpy, вывод чисел с 6 знаками после запятой
pd.set_option('display.precision', 6)           #То же, что и предыдущее

# Шаг 1. Укажите путь к вашему CSV и нужные колонки
# Заполните переменные ниже: CSV_PATH, FEATURE_COLS (список признаков), TARGET_COL (целевой).
CSV_PATH = 'dataset.xlsx'  # путь к файлу с данными пингвинов
FEATURE_COLS = ['x1', 'x2', 'x3', 'x4']  # признаки: x1=длина клюва, x2=глубина клюва, x3=длина плавника, x4=масса тела
TARGET_COL = 'y'  # целевая переменная: y=вид пингвина

# Шаг 2. Загрузка данных и базовая проверка
df = pd.read_excel(CSV_PATH)
assert all(col in df.columns for col in FEATURE_COLS), 'Не найдены все признаки в таблице'  # assert проверяет истинность условия
assert TARGET_COL in df.columns, 'Не найден столбец TARGET_COL'
# Попробуем привести y к числу (если классы записаны строками, преобразуйте их заранее)
df[TARGET_COL] = pd.to_numeric(df[TARGET_COL], errors='coerce')
assert df[TARGET_COL].notna().all(), 'В y должны быть числа (при необходимости предварительно закодируйте классы)'
df.head()

# Шаг 3. Собираем матрицы X, y и обучаем LinearRegression (= LINEST)
# Получите ŷ и остатки resid = y - ŷ.
# Формируем матрицу признаков X из выбранных столбцов и преобразуем в numpy массив
X = df[FEATURE_COLS].to_numpy()  # Берем колонки x1, x2, x3, x4 и превращаем в числовую матрицу
# Формируем вектор целевой переменной y и преобразуем в float
y = df[TARGET_COL].to_numpy().astype(float)  # Берем колонку y (вид пингвина) и делаем числами
# Создаем и обучаем модель линейной регрессии с включением свободного члена
model = LinearRegression(fit_intercept=True).fit(X, y)  # Обучаем модель на данных X и y
# Получаем предсказанные значения ŷ с помощью обученной модели
y_hat = model.predict(X)  # Модель предсказывает вид пингвина по признакам
# Вычисляем остатки - разницу между реальными и предсказанными значениями
resid = y - y_hat  # Остатки = настоящий вид минус предсказанный вид

# Самопроверка A
n, k = X.shape  # Получаем размеры матрицы X: n - количество строк (наблюдений), k - количество столбцов (признаков)
assert y.shape == (n,), 'y должен быть вектором длины n'  # Проверяем, что размер вектора y совпадает с количеством наблюдений
assert k >= 1, 'Нужно минимум 1 признак'  # Проверяем, что в модели есть хотя бы один признак
print('OK: n=%d, k=%d' % (n, k))  # Выводим информацию о размерах данных: количество наблюдений и признаков

# Шаг 4. Базовые суммы квадратов и метрики LINEST
# Посчитайте:

# SStot = Σ (y - ȳ)² - общая сумма квадратов (общая изменчивость данных) (y факт - ср знач y)
SStot = ((y - y.mean())**2).sum()  # Сумма квадратов отклонений реальных значений от среднего

# SSresid = Σ (y - ŷ)² - сумма квадратов остатков (необъясненная моделью изменчивость)
SSresid = ((y - y_hat)**2).sum()  # Сумма квадратов ошибок предсказания

# SSreg = SStot - SSresid - разница между общей и остаточной изменчивостью (объясненная моделью изменчивость)
SSreg = SStot - SSresid  # Часть изменчивости, которую смогла объяснить модель

# R² = 1 - SSresid/SStot - коэффициент детерминации (доля объясненной дисперсии)
R2 = 1 - SSresid / SStot  # Показывает, насколько хорошо модель описывает данные (от 0 до 1)

# df = n - k - 1 - степени свободы остатков
df_resid = n - k - 1  # Количество свободно изменяемых наблюдений после учета параметров модели

# SE_y = √(SSresid/df) - стандартная ошибка оценки
SE_y = np.sqrt(SSresid / df_resid)  # Средняя ошибка предсказания модели

# F = (SSreg/k)/(SSresid/df) - F-статистика для проверки значимости модели
F = (SSreg / k) / (SSresid / df_resid)  # Проверяет, значима ли модель в целом
print("Метрики модели рассчитаны:")
print(f"SStot = {SStot:.2f}")      # Общая изменчивость
print(f"SSreg = {SSreg:.2f}")      # Объясненная изменчивость
print(f"SSresid = {SSresid:.2f}")  # Необъясненная изменчивость
print(f"R² = {R2:.4f}")           # Качество модели (0-1)
print(f"Степени свободы = {df_resid}")  # Надежность оценок
print(f"SE_y = {SE_y:.2f}")       # Средняя ошибка предсказания
print(f"F-статистика = {F:.2f}")  # Значимость модели
#ПРОВЕРОЧКА
assert df_resid == n - k - 1, 'df должно быть n - k - 1'
assert SStot >= 0 and SSresid >= 0 and SSreg >= 0, 'Суммы квадратов не могут быть отрицательными'
assert 0 <= R2 <= 1 or np.isclose(R2, 1.0) or np.isclose(R2, 0.0), 'R^2 должен быть в [0,1] на тренировочных данных'
print('OK')

# Шаг 5. SE коэффициентов (2-я строка LINEST)
# Соберите дизайн-матрицу X1 = [X, 1] и используйте формулу:
# Var(β̂) = σ²(X₁ᵀX₁)⁻¹, где σ² = SS_resid/df

# Создаем расширенную матрицу признаков X1, добавляя столбец единиц для свободного члена, т.е. b
X1 = np.column_stack([X, np.ones(n)])  # Дизайн-матрица: [признаки, единицы]

# Вычисляем матрицу X1 транспонированную умноженную на X1
XtX = X1.T @ X1  # Матрица скалярных произведений признаков

# Пытаемся найти обратную матрицу (X1ᵀX1)⁻¹
try:
    XtX_inv = np.linalg.inv(XtX)  # Стандартное обращение матрицы
except np.linalg.LinAlgError:
    XtX_inv = np.linalg.pinv(XtX)  # Псевдообратная матрица, если матрица вырождена

# Вычисляем оценку дисперсии ошибок σ²
sigma2 = SSresid / df_resid  # Несмещенная оценка дисперсии: остаточная сумма квадратов / степени свободы

# Вычисляем стандартные ошибки коэффициентов как корень из диагональных элементов ковариационной матрицы
se_beta = np.sqrt(np.diag(sigma2 * XtX_inv))  # SE(b) = √[σ² · diag((X₁ᵀX₁)⁻¹)]

print("Стандартные ошибки коэффициентов:")
print(f"SE для x1 (длина клюва): {se_beta[0]:.6f}")
print(f"SE для x2 (глубина клюва): {se_beta[1]:.6f}")
print(f"SE для x3 (длина плавника): {se_beta[2]:.6f}")
print(f"SE для x4 (масса тела): {se_beta[3]:.6f}")
print(f"SE для intercept (b): {se_beta[4]:.6f}")

# Шаг 6. Собираем пятирядный блок как в Excel
# 1-я строка: коэффициенты {m_k, …, m_1, b0} (обратите внимание на обратный порядок признаков, как в LINEST)
# 2-я: SE в том же порядке
# 3-я: {R², SE_y, NA, NA, NA}
# 4-я: {F, df, NA, NA, NA}
# 5-я: {SSreg, SSresid, NA, NA, NA}

# Получаем коэффициенты модели и свободный член
coefs = getattr(model, 'coef_', None)  # Коэффициенты при признаках [m1, m2, m3, m4], getattr вытаскивает из машинной модели наши коэффициенты
intercept = getattr(model, 'intercept_', None)  # Свободный член β₀

# Создаем DataFrame с 5 строками и k+1 столбцами (4 признака + intercept)
linest_like = pd.DataFrame(index=[1,2,3,4,5], columns=[f'c{i+1}' for i in range(k+1)])

# Заполняем строки 1 и 2 с учетом обратного порядка коэффициентов (как в Excel LINEST)
rev_coefs = list(coefs[::-1]) + [intercept]  # Коэффициенты в обратном порядке: [m4, m3, m2, m1, b]
rev_se = list(se_beta[:k][::-1]) + [se_beta[-1]]  # SE коэффициентов в том же порядке: [SE(m4), SE(m3), SE(m2), SE(m1), SE(b)]

# Записываем коэффициенты и их стандартные ошибки
linest_like.loc[1, :] = rev_coefs  # Строка 1: коэффициенты регрессии
linest_like.loc[2, :] = rev_se     # Строка 2: стандартные ошибки коэффициентов

# Строки 3–5: только первые 2 столбца, остальные — NaN (как #Н/Д в Excel)
linest_like.loc[3, :] = [R2, SE_y] + [np.nan] * (k-1)  # Строка 3: R^2 и стандартная ошибка модели
linest_like.loc[4, :] = [F, df_resid] + [np.nan] * (k-1)  # Строка 4: F-статистика и степени свободы
linest_like.loc[5, :] = [SSreg, SSresid] + [np.nan] * (k-1)  # Строка 5: суммы квадратов
print("Пятирядный блок в формате Excel LINEST:")
print(linest_like)

# Шаг 7. (Опционально) Если y — целые классы, округлите ŷ и посчитайте accuracy
# Установите TREAT_AS_CLASSES = True, если хотите режим «классы через округление». Если у вас обычная регрессия — оставьте False.

TREAT_AS_CLASSES = True  # Меняем на True, так как у меня y — целые классы (виды пингвинов 1,2,3)

if TREAT_AS_CLASSES:
    # Определяем минимальное и максимальное значение классов
    y_min, y_max = int(np.nanmin(y)), int(np.nanmax(y))  # y_min=1, y_max=3 (виды пингвинов)
    y_pred_class = np.clip(np.rint(y_hat), y_min, y_max).astype(
        int)  # Округляем ŷ к ближайшему целому и обрезаем до диапазона [1,3], ошибку для внимательных учли, np.clip() — это функция в библиотеке NumPy для Python, которая обрезает значения в массиве или отдельном числе, чтобы они находились в заданном диапазоне от минимального до максимального значения.
    # Вычисляем точность (accuracy) - доля правильных предсказаний
    acc = accuracy_score(y.astype(int), y_pred_class)  # Сравниваем реальные классы с предсказанными

    print('Accuracy:', round(acc, 4))  # Выводим точность с округлением до 4 знаков

#Шаг 8. (Опционально) Матрица ошибок и два графика
if TREAT_AS_CLASSES:
    cm = confusion_matrix(y.astype(int), y_pred_class, labels=sorted(np.unique(y_pred_class)))
    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=sorted(np.unique(y_pred_class)))
    disp.plot(); plt.title('Матрица ошибок'); plt.show()

plt.figure(); plt.scatter(y, y_hat); mn, mx = np.nanmin([y.min(), y_hat.min()]), np.nanmax([y.max(), y_hat.max()]); plt.plot([mn,mx],[mn,mx]); plt.xlabel('y'); plt.ylabel('ŷ'); plt.title('y vs ŷ'); plt.show()
plt.figure(); plt.hist(y - y_hat, bins=20); plt.xlabel('Остаток'); plt.ylabel('Частота'); plt.title('Распределение остатков'); plt.show()